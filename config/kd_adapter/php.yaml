Annotation:
  device: v1008 
  language: en-de
  dataset: php
  what: KD php Adapter from kde 
  hyper_parameter: 
  log_file: 

Dataset:
  train_dataset_path:

    - /home/zhengx/data/wmt14_adapt_dataset_share/php-clean/php-train.tsv  # php

  train_dataset_domain: [php]

  dev_dataset_path:

    - /home/zhengx/data/wmt14_adapt_dataset_share/php-clean/php-dev.tsv  # php

  dev_dataset_domain: [php]

  test_dataset_path:
    - /home/zhengx/data/wmt14_adapt_dataset_share/kde-clean/kde-dev.tsv
    - /home/zhengx/data/wmt14_adapt_dataset_share/php-clean/php-dev.tsv
    - /home/zhengx/data/wmt14_adapt_dataset_share/ubuntu-clean/ubuntu-dev.tsv
    - /home/zhengx/data/wmt14_adapt_dataset_share/GNOME-clean/GNOME-dev.tsv

  combine_train_datasets: False
  filter_len: 200

Vocab:
  use_bpe: True
  share_bpe:
  src:
    file: /home/zhengx/data/wmt14_adapt_dataset_share/vocab
    max_size: 37000
  trg:
    file: /home/zhengx/data/wmt14_adapt_dataset_share/vocab
    max_size: 37000

Model:
  feature_size: 512
  feedforward_dim: 2048
  head_num: 8
  dropout_rate: 0.1
  num_layers: 6
  layer_norm_rescale: True

  share_enc_dec_embedding: True
  share_decoder_embedding: True

  # generator
  generator_bias: False

  classifier_type: simple
  classify_feature_size: 256

  domain_class_num: 15
  classify_domain_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
  domain_dict: {
    'news': 0,
    'iwslt': 1,
    'books': 2,
    'bible': 3,
    'laws': 4,
    'medical': 5,
    'koran': 6,
    'ubuntu': 7,
    'kde': 8,
    'GNOME': 9,
    'php': 10
  }

  domain_adapter_dict: {
    'kde': {
      'memory_count': 512
    },
    'php': {
      'memory_count': 128
    }
  }

  domain_inner_gate_list: []

  adapter_setting: {
    'fusion': mix,
    'type': stack,
  }

  domain_list: [news, iwslt, books, bible, laws, medical, koran, ubuntu, kde, GNOME, php]

Criterion:
  # name: cross_entropy
  # name: nll
  # label_smoothing: 0.1
  # reduction: mean

  name: kl_divergence
  label_smoothing: 0.1
  reduction: sum

Optimizer:
  beta2: 0.998
  lr_rate: 5.0e-4
  lr_scheduler:
    # name: reduce_lr_on_bleu
    # factor: 0.5
    # patience: 5
    # min_lr: 5.0e-5
    name: noam
    factor: 2.0
    warmup_steps: 8000
    model_size: 512
  grad_clip: -1.0

Train:
  stage: kd
  target_domain: php
  kd_ref_domain: {
    'kde': {
      'temperature': 4,
      'factor': 1.0
    },
  }

  used_inner_gate: True

  use_multiple_gpu: False

  random_seed: 1234
  load_exist_model: True # load model
  model_load_path: /home/data_ti5_c/zhengx/model_save/mix-adapter/kde/512-noam-labelsmooth/bleu_best/model
  load_optimizer: False  # load model and optimizer
  optimizer_path: 
  load_lr_scheduler: False
  lr_scheduler_path: 

  batch_size: 6000
  update_batch_count: 2
  epoch_num: 10000

Validation:

  batch_size: 32

  loss_validation:
    start_on_steps: 1000
    frequency: 1000
  
  bleu_validation:
    start_on_steps: 1000
    frequency: 1000

    tokenize: intl
    target_language: de
    use_bpe: True
    ref: /home/zhengx/data/wmt14_adapt_dataset_share/php-clean/php-dev.de
    detruecase_script: /home/user_data55/zhengx/project/NMT/scripts/recaser/detruecase.perl
    detokenize_script: /home/user_data55/zhengx/project/NMT/scripts/tokenizer/detokenizer.perl

    beam_search:
      beam_size: 4
      max_steps: 150
      length_penalty: True
      alpha: 0.6

Test:
  model_name: transformer_with_mix_adapter
  target_domain: news
  refs: 
    - /home/zhengx/data/wmt14_adapt_dataset_share/kde-clean/kde-dev.de
    - /home/zhengx/data/wmt14_adapt_dataset_share/php-clean/php-dev.de
    - /home/zhengx/data/wmt14_adapt_dataset_share/ubuntu-clean/ubuntu-dev.de
    - /home/zhengx/data/wmt14_adapt_dataset_share/GNOME-clean/GNOME-dev.de

  output_path:
    - /home/zhengx/output/mix-ende/kde.output
    - /home/zhengx/output/mix-ende/php.output
    - /home/zhengx/output/mix-ende/ubuntu.output
    - /home/zhengx/output/mix-ende/GNOME.output

  model_path: /home/data_ti5_c/zhengx/model_save/mix-adapter/kde512-php128-ubuntu128-gnome512/model
  
  save_in_multi_gpu: False
  tokenize: intl
  target_language: de
  use_bpe: True
  detruecase_script: /home/user_data55/zhengx/project/DomainAdaptationForTranslation/scripts/recaser/detruecase.perl
  detokenize_script: /home/user_data55/zhengx/project/DomainAdaptationForTranslation/scripts/tokenizer/detokenizer.perl

  batch_size: 128
  tag: translation

  beam_search:
    beam_size: 4
    max_steps: 150
    length_penalty: True
    alpha: 0.6


Record:
  training_record_path: /home/data_ti5_c/zhengx/record/mix-adapter/php/128-kd_kde-t4-f10
  
  model_record:
    path: /home/zhengx/model_save/mix-adapter/php/128-kd_kde-t4-f10
    best_model_save:
      loss_best: False
      bleu_best: True
      save_optimizer: False
      save_lr_scheduler: False
    
    # save last checkpoint2
    last_checkpoint_save:
      start_on_steps: 50000
      frequency: 2000
      save_optimizer: False
      save_lr_scheduler: False
      save_checkpoint_count: 0

visualize_file: /home/zhengx/visualize/wmt14_adapt/news
