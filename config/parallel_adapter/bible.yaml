Annotation:
  device: v1007
  language: en-de
  dataset: bible
  what: 
  hyper_parameter: 
  log_file: 

Dataset:
  train_dataset_path:
    - /home/zhengx/data/wmt14_adapt_dataset_share/bible-clean/bible-train.tsv

  dev_dataset_path:
    - /home/zhengx/data/wmt14_adapt_dataset_share/bible-clean/bible-dev.tsv

  test_dataset_path:
    - /home/zhengx/data/wmt14_adapt_dataset_share/bible-clean/bible-dev.tsv

  combine_train_datasets: False
  filter_len: 200

Vocab:
  use_bpe: True
  src:
    file: /home/zhengx/data/wmt14_adapt_dataset_share/vocab
    max_size: 37000
  trg:
    file: /home/zhengx/data/wmt14_adapt_dataset_share/vocab
    max_size: 37000

Model:
  feature_size: 512
  feedforward_dim: 2048
  head_num: 8
  dropout_rate: 0.1
  num_layers: 6
  layer_norm_rescale: True

  share_enc_dec_embedding: True
  share_decoder_embedding: True

  domain_adapter_dict: {
    # 'iwslt': {
    #   'memory_count': 384
    # },
    'bible': {
      'memory_count': 1024,
      'bias_for_other_adapter': True,
      'bias_type': 'dense',
      'rank': 4,
    },
  }
  max_domain_num: 20
  domain_idx_dict: {
    'iwslt': 1,
    'bible': 2,
  }
  adapter_weighted_sum: False

  # generator
  generator_bias: False

Criterion:
  # name: nll
  # label_smoothing: 0.1
  # reduction: mean
  name: kl_divergence
  label_smoothing: 0.1
  reduction: sum

Optimizer:
  beta2: 0.998
  lr_rate: 5.0e-4
  lr_scheduler:
    name: reduce_lr_on_bleu
    factor: 0.5
    patience: 5
    min_lr: 5.0e-5
  # lr_scheduler:
  #   name: noam
  #   factor: 2.0
  #   model_size: 512
  #   warmup_steps: 8000
  grad_clip: -1.0


Train:
  training_domain: bible
  used_domain_list: ['bible']
  use_multiple_gpu: False

  random_seed: 1234
  load_exist_model: True
  # model_load_path: /home/data_ti4_c/zhengx/model_save/wmt14-ende/no_layer_norm_rescale/loss_best/model
  # model_load_path: /home/zhengx/model_save/wmt14-parallel-adapter/bible/iwslt_only-train-bible-512/bleu_best/model
  model_load_path: /home/zhengx/model_save/wmt2014-ende/6000-4/loss_best_model
  load_optimizer: False  
  optimizer_path:
  load_lr_scheduler: False
  lr_scheduler_path: 

  batch_size: 12000
  update_batch_count: 1
  epoch_num: 500

Validation:

  batch_size: 64

  loss_validation:
    start_on_steps: 1000
    frequency: 1000
  
  bleu_validation:
    start_on_steps: 1000
    frequency: 1000

    tokenize: intl
    target_language: de
    use_bpe: True
    ref: /home/zhengx/data/wmt14_adapt_dataset_share/bible-clean/bible-dev.de
    detruecase_script: /home/user_data55/zhengx/project/NMT/scripts/recaser/detruecase.perl
    detokenize_script: /home/user_data55/zhengx/project/NMT/scripts/tokenizer/detokenizer.perl

    beam_search:
      beam_size: 4
      max_steps: 150
      length_penalty: True
      alpha: 0.6

Test:
  refs: 
    # - /home/zhengx/data/wmt14_adapt_dataset_share/iwslt2015/IWSLT15.TED.tst2013.en-de.de
    # - /home/zhengx/data/wmt14_adapt_dataset_share/iwslt2015/IWSLT15.TED.tst2014.en-de.de
    # - /home/zhengx/data/wmt2014-ende-share/newstest2014.de
    - /home/zhengx/data/wmt14_adapt_dataset_share/bible/bible-dev.de

  output_path:
    # - /home/zhengx/output/iwslt2015/tst2013.output
    # - /home/zhengx/output/iwslt2015/tst2014.output
    # - /home/zhengx/output/iwslt2015/news2014.output
    - /home/zhengx/output/bible/bible.output

  save_in_multi_gpu: False
  model_path: /home/zhengx/model_save/wmt14-parallel-adapter/bible/iwslt-adapter-512/bleu_best/model

  tokenize: intl
  target_language: de
  use_bpe: True
  detruecase_script: /home/user_data55/zhengx/project/NMT/scripts/recaser/detruecase.perl
  detokenize_script: /home/user_data55/zhengx/project/NMT/scripts/tokenizer/detokenizer.perl

  batch_size: 128
  tag: translation
  used_domain_list: ['bible']

  beam_search:
    beam_size: 4
    max_steps: 150
    length_penalty: True
    alpha: 0.6

Record:
  training_record_path: /home/user_data55/zhengx/record/wmt14-parallel-adapter/bible/adapter-1024-dense-4
  
  model_record:
    path: /home/zhengx/model_save/wmt14-parallel-adapter/bible/adapter-1024-dense-4
    best_model_save:
      loss_best: False
      bleu_best: True
      save_optimizer: False
      save_lr_scheduler: False
    
    # save last checkpoint
    last_checkpoint_save:
      start_on_steps: 50000
      frequency: 2000
      save_optimizer: False
      save_lr_scheduler: False
      save_checkpoint_count: 0
